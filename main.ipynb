{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For vectorization use **bert** then you can use \n",
    "    1. Cosine similarity \n",
    "    2. euclidean distance\n",
    "\n",
    "2. **Semantic Textual Similarity (STS) Metrics**: These metrics, such as Pearson correlation or Spearman rank correlation, are designed specifically for assessing the semantic similarity between two pieces of text. They are often used in NLP benchmarks.\n",
    "\n",
    "3. **BLEU (Bilingual Evaluation Understudy)**: Originally designed for machine translation evaluation, BLEU measures the n-gram overlap between the generated sentence and a reference (paragraph in this case). It can be adapted for similarity by treating the paragraph as a reference.\n",
    "\n",
    "4. **Word Mover's Distance (WMD)**: WMD calculates the minimum cumulative distance that words in one document need to travel to match the words in another document. It considers semantic similarity and can be effective for measuring the distance between a sentence and a paragraph.\n",
    "\n",
    "5. **Fine-Tuned Models**: Alternatively, you can use fine-tuned BERT models that are specifically trained for sentence \n",
    "similarity tasks. These models may provide more accurate similarity scores tailored to your specific application.\n",
    "\n",
    "6. **Sentence Transformers**: There are libraries and models built on top of BERT, such as Sentence Transformers, which provide ready-to-use implementations for computing sentence embeddings and similarity scores. These models are trained to generate semantically meaningful sentence embeddings that capture similarity effectively.\n",
    "\n",
    "7. **Jaccard metrics**: It can only be used incase we have bag of words so that binary vector is created for each sentence.\n",
    "\n",
    "8. **BLEU (Bilingual Evaluation Understudy):** Originally designed for machine translation evaluation, BLEU measures the n-gram overlap between the generated sentence and a reference (paragraph in this case). It can be adapted for similarity by treating the paragraph as a reference.\n",
    "\n",
    "9. **Semantic Textual Similarity (STS) Metrics:** These metrics, such as Pearson correlation or Spearman rank correlation, are designed specifically for assessing the semantic similarity between two pieces of text. They are often used in NLP benchmarks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity between the sentences: 0.5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Implementation of jaccard simillary metrics calculation\"\"\"\n",
    "\"\"\"\n",
    "formula to calculate Jaccard simmilarity = a(intersection)b / a(union)b\n",
    "\"\"\"\n",
    "def calculate_jaccard_similarity(sentence1, sentence2):\n",
    "    words_set1 = set(sentence1.lower().split())\n",
    "    words_set2 = set(sentence2.lower().split())\n",
    "    \n",
    "    intersection = len(words_set1.intersection(words_set2))\n",
    "    union = len(words_set1.union(words_set2))\n",
    "    jaccard_similarity = intersection / union if union != 0 else 0\n",
    "    \n",
    "    return jaccard_similarity\n",
    "\n",
    "sentence1 = \"The cat is on the mat\"\n",
    "sentence2 = \"The mat has a cat on it\"\n",
    "similarity_score = calculate_jaccard_similarity(sentence1, sentence2)\n",
    "print(f\"Jaccard Similarity between the sentences: {similarity_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between the sentences: 0.8610\n",
      "start_time = 1710831797.6821022\n",
      "model_fetched_time = 1.2878665924072266\n",
      "tokenized_time = 1710831797.783492\n",
      "cosine_similarity_time = 1.2878665924072266\n"
     ]
    }
   ],
   "source": [
    "\"\"\"implementation of cosine simmilarity using bert for vectorization\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "\n",
    "model_fetched_time = time.time() - start_time\n",
    "\n",
    "# Define two sentences for similarity comparison\n",
    "sentence1 = \"The cat is on the mat.\"\n",
    "sentence2 = \"The mat has a cat on it.\"\n",
    "\n",
    "# Tokenize and encode the sentences using BERT tokenizer\n",
    "inputs1 = tokenizer(sentence1, return_tensors='pt', padding=True, truncation=True)\n",
    "inputs2 = tokenizer(sentence2, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Get BERT embeddings for the sentences\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs1)\n",
    "    outputs2 = model(**inputs2)\n",
    "\n",
    "# Extract the last layer hidden states (word embeddings) for each sentence\n",
    "last_hidden_states1 = outputs1.last_hidden_state\n",
    "last_hidden_states2 = outputs2.last_hidden_state\n",
    "\n",
    "# Average the word embeddings across each sentence to get sentence embeddings\n",
    "sentence_embedding1 = torch.mean(last_hidden_states1, dim=1).squeeze()\n",
    "sentence_embedding2 = torch.mean(last_hidden_states2, dim=1).squeeze()\n",
    "\n",
    "tokenized_time = time.time() - model_fetched_time \n",
    "\n",
    "# Convert sentence embeddings to numpy arrays and calculate cosine similarity\n",
    "embedding1_np = sentence_embedding1.numpy().reshape(1, -1)\n",
    "embedding2_np = sentence_embedding2.numpy().reshape(1, -1)\n",
    "cos_sim = cosine_similarity(embedding1_np, embedding2_np)[0][0]\n",
    "\n",
    "cosine_similarity_time = time.time() - tokenized_time\n",
    "\n",
    "print(f\"Cosine Similarity between the sentences: {cos_sim:.4f}\")\n",
    "print(f\"start_time = {start_time}\\nmodel_fetched_time = {model_fetched_time}\\ntokenized_time = {tokenized_time}\\ncosine_similarity_time = {cosine_similarity_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.76337683]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Used transformer for embedding sentence in to one dimension vector\"\"\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "sentence = ['One cat is siting here', 'Here is one cat sitting on mat']\n",
    "embedding = model.encode(sentence)\n",
    "a = embedding[0].reshape(1,-1)\n",
    "b = embedding[1].reshape(1,-1)\n",
    "print(cosine_similarity(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17073156633678052\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "dist = distance.euclidean(embedding[0],embedding[1])\n",
    "similarity = 1/(1+dist)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012690662015076357\n"
     ]
    }
   ],
   "source": [
    "manhattan_distance = distance.cityblock(embedding[0],embedding[1])\n",
    "similarity_score = 1 / (1 + manhattan_distance)\n",
    "print(similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(binary_array1, binary_array2):\n",
    "    set1 = set(binary_array1.nonzero()[0])  # Convert non-zero indices to a set\n",
    "    set2 = set(binary_array2.nonzero()[0])  # Convert non-zero indices to a set\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    similarity_score = intersection / union if union != 0 else 0  # Handle division by zero\n",
    "    return similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentences = ['One cat is siting here', 'Here is one cat sitting on mat']\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(sentences)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(jaccard_similarity(bow_matrix.toarray()[0],bow_matrix.toarray()[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
